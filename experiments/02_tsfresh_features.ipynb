{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tsfresh features\n",
    "\n",
    "MetaOD approach uses non-time series specific meta features. As it is an important part of the meta learning pipeline, time series specific features are extracted using the tsfresh package. Tsfresh is a python package. It automatically calculates a large number of time series features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tsfresh\n",
    "from tsfresh.feature_extraction import (\n",
    "    extract_features,\n",
    "    EfficientFCParameters,\n",
    "    ComprehensiveFCParameters,\n",
    "    MinimalFCParameters,\n",
    "\n",
    ")\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all datasets, features were extracted using the `EfficientFCParameters` setting. Using `ComprehensiveFCParameters` adds 2 features at many times higher computation cost. `EfficientFCParameters` extract more than 700 time series specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approximate_entropy', 'sample_entropy'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ComprehensiveFCParameters()) - set(EfficientFCParameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:13<00:00, 11.41it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/train.txt') as f:\n",
    "    train = f.readlines()\n",
    "\n",
    "with open('data/test.txt') as f:\n",
    "    test = f.readlines()\n",
    "\n",
    "files = train + test\n",
    "settings = EfficientFCParameters()\n",
    "\n",
    "\n",
    "def get_features(file):\n",
    "    file = file.strip('\\n')\n",
    "    ts = np.loadtxt(f'./data/datasets/{file}')\n",
    "    df = pd.DataFrame(ts, columns=['value'])\n",
    "    df['id'] = file\n",
    "    if not os.path.exists(f'./data/datasets/metafeatures/tsfresh_{file}.npy'):\n",
    "        features = extract_features(\n",
    "            df,\n",
    "            column_id='id',\n",
    "            column_value='value',\n",
    "            n_jobs=40,\n",
    "            default_fc_parameters=settings,\n",
    "        )\n",
    "        np.save(\n",
    "            f'./data/datasets/metafeatures/tsfresh_{file}',\n",
    "            features.values.squeeze(),\n",
    "        )\n",
    "    return\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for file in tqdm.tqdm(files):\n",
    "    dfs.append(get_features(file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 1/1 [00:03<00:00,  3.35s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:03<00:00,  3.70s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:03<00:00,  3.25s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:03<00:00,  3.49s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:03<00:00,  3.53s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:04<00:00,  4.10s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:04<00:00,  4.62s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:03<00:00,  3.78s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:03<00:00,  3.71s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:06<00:00,  6.43s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [01:49<00:00, 109.80s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:08<00:00,  8.98s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:04<00:00,  4.70s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:02<00:00,  2.80s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [01:19<00:00, 79.16s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [01:13<00:00, 73.26s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [01:06<00:00, 66.28s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:41<00:00, 41.34s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:47<00:00, 47.68s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [01:02<00:00, 62.96s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:41<00:00, 41.98s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [01:01<00:00, 61.43s/it]\n",
      "Feature Extraction: 100%|██████████| 1/1 [00:37<00:00, 37.74s/it]\n",
      "100%|██████████| 32/32 [12:16<00:00, 23.01s/it]\n"
     ]
    }
   ],
   "source": [
    "with open('./data/datasets/numenta/combined_labels.json', 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "labels = {k.split('/')[1]: v for k, v in labels.items()}\n",
    "\n",
    "files = list(labels.keys())\n",
    "settings = EfficientFCParameters()\n",
    "\n",
    "\n",
    "def get_features(file):\n",
    "    file = file.strip('\\n')\n",
    "    ts = pd.read_csv(f'./data/datasets/numenta/{file}')\n",
    "    ts['id'] = file\n",
    "    df = ts[['value', 'id']]\n",
    "    if not os.path.exists(\n",
    "        f'./data/datasets/numenta/metafeatures/tsfresh_{file}.npy'\n",
    "    ):\n",
    "        features = extract_features(\n",
    "            df,\n",
    "            column_id='id',\n",
    "            column_value='value',\n",
    "            n_jobs=40,\n",
    "            default_fc_parameters=settings,\n",
    "        )\n",
    "        np.save(\n",
    "            f'./data/datasets/numenta/metafeatures/tsfresh_{file}',\n",
    "            features.values.squeeze(),\n",
    "        )\n",
    "    return\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for file in tqdm.tqdm(files):\n",
    "    dfs.append(get_features(file))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automltsad-m3CR1GKj-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (main, Mar 30 2022, 15:42:06) [GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "753c5367c7457dfba345b6d1c78053c473309d2e4cf887d946f7023df441abb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
