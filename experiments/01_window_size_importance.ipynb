{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/automltsad-DuHKdg10-py3.10/lib/python3.10/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "from automltsad.utils import (\n",
    "    Autoperiod,\n",
    "    sliding_window_sequences,\n",
    "    to_time_series_dataset,\n",
    "    reduce_window_scores\n",
    ")\n",
    "from automltsad.detectors import LOF, KNN, IsolationForestAD\n",
    "from automltsad.metrics import precision_recall_curve, f1_pa, f1_pa_t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.txt') as f:\n",
    "    datasets = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['196_UCR_Anomaly_sel840mECG2_20000_49370_49740.txt', '022_UCR_Anomaly_DISTORTEDGP711MarkerLFM5z4_4000_6527_6645.txt', '091_UCR_Anomaly_DISTORTEDtiltAPB3_40000_114000_114370.txt', '227_UCR_Anomaly_mit14134longtermecg_11231_29000_29100.txt', '149_UCR_Anomaly_Lab2Cmac011215EPG5_7000_17390_17520.txt', '054_UCR_Anomaly_DISTORTEDWalkingAceleration5_2700_5920_5979.txt', '151_UCR_Anomaly_MesoplodonDensirostris_10000_19280_19440.txt', '104_UCR_Anomaly_NOISEapneaecg4_6000_16000_16100.txt', '030_UCR_Anomaly_DISTORTEDInternalBleeding19_3000_4187_4197.txt', '168_UCR_Anomaly_gait2_22000_46500_46800.txt']\n"
     ]
    }
   ],
   "source": [
    "# print(np.random.choice(list(range(len(datasets))), 10, False))\n",
    "dataset_idx = [18, 137,  71,  85,  35, 126,  20,  75,   6,  43]\n",
    "dataset_names = [datasets[i].strip('\\n') for i in dataset_idx]\n",
    "print(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, n in enumerate(dataset_names):\n",
    "def load_dataset(filename):\n",
    "    test_start, anomaly_start, anomaly_end = [\n",
    "        int(i) for i in filename.split('.')[0].split('_')[-3:]\n",
    "    ]\n",
    "    ts = np.loadtxt(f'./data/datasets/{filename}')\n",
    "    ts = to_time_series_dataset(ts)\n",
    "    train = ts[:,:test_start]\n",
    "    test = ts[:,test_start:]\n",
    "    label = np.zeros_like(test)\n",
    "    label[:,anomaly_start-test_start:anomaly_end-test_start] = 1\n",
    "    label = np.squeeze(label)\n",
    "    return train, test, label, test_start, anomaly_start, anomaly_end\n",
    "\n",
    "def window_data(train, test, size):\n",
    "    train_w = sliding_window_sequences(train, size)\n",
    "    test_w = sliding_window_sequences(test, size)\n",
    "    return train_w, test_w\n",
    "\n",
    "id_to_detector = dict(\n",
    "    knn=KNN,\n",
    "    lof=LOF,\n",
    "    IF=IsolationForestAD\n",
    ")\n",
    "\n",
    "hyperparams = {\n",
    "    'knn':{\n",
    "        'n_neighbors':15\n",
    "    },\n",
    "    'lof':{}, # Default\n",
    "    'IF':{} # Default\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = ['knn', 'lof', 'IF']\n",
    "window_sizes = list(range(8,513,8))\n",
    "f1_scores = np.zeros((len(detectors), len(dataset_names), len(window_sizes)))\n",
    "f1_wo_pa_scores = np.zeros((len(detectors), len(dataset_names), len(window_sizes)))\n",
    "f1_pa_scores = np.zeros((len(detectors), len(dataset_names), len(window_sizes)))\n",
    "\n",
    "configurations = []\n",
    "for i, det_id in enumerate(detectors):\n",
    "    for j, fn in enumerate(dataset_names):\n",
    "        for k, sz in enumerate(window_sizes): # Window sizes\n",
    "            configurations.append((i,j,k,det_id,fn,sz))\n",
    "\n",
    "def process_task(config):\n",
    "    i, j, k, det_id, fn, sz = config\n",
    "    train, test, label, test_start, anomaly_start, anomaly_end = load_dataset(fn)\n",
    "    train_w, test_w = window_data(train, test, sz)\n",
    "    detector = id_to_detector[det_id]\n",
    "    model = detector(**hyperparams[det_id])\n",
    "    model.fit(train_w)\n",
    "    scores = model.predict_anomaly_scores(test_w)\n",
    "    scores = reduce_window_scores(scores, sz)\n",
    "    p, r, t = precision_recall_curve(label, scores)\n",
    "    f1 = 2*p*r / (p+r+1e-12)\n",
    "    idx = np.nanargmax(f1)\n",
    "    step = max(len(t)//100,1)\n",
    "    f1_pas = [f1_pa(label, scores, th) for th in t[::step]]\n",
    "    return i,j, k,f1[idx], f1_pa(label, scores, t[idx]), np.max(f1_pas)\n",
    "\n",
    "with Pool(32) as p:\n",
    "    for res in p.map(process_task, configurations, chunksize=10):\n",
    "        i,j,k,f1,f1pat,f1pa = res\n",
    "        f1_scores[i, j, k]=f1\n",
    "        f1_wo_pa_scores[i, j, k]=f1pat\n",
    "        f1_pa_scores[i, j, k]=f1pa\n",
    "np.savez('./results/window_size_experiment_results', f1_scores=f1_scores, f1_wo_pa_scores=f1_wo_pa_scores, f1_pa_scores=f1_pa_scores)\n",
    "\n",
    "# window_ap = np.zeros(len(dataset_names))\n",
    "# for j, fn in enumerate(tqdm(dataset_names)):\n",
    "#     train, _, _, _, _, _ = load_dataset(fn)\n",
    "#     window_ap[j] = Autoperiod(np.squeeze(train)).period\n",
    "# np.save('./results/window_size_experiment_autoperiod', window_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./results/window_size_experiment_results.npz')\n",
    "dataset_period = np.load('./results/window_size_experiment_autoperiod.npy')\n",
    "f1_s, f1_wo_pa, f1_pa = data['f1_scores'], data['f1_wo_pa_scores'], data['f1_pa_scores']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automltsad-DuHKdg10-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34922e479d629e8a37591713d52f461334259fb96d0b4427dce13d4d84044151"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
